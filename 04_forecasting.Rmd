---
title: "04_forecasting"
---
---
title: "Forecasting"
---

```{r, echo=FALSE}
#install.packages(c("readr", "data.table", "psych","dplyr", "tidyr", "reshape2", "lubridate", "forecast", "astsa", "zoo", "nowcasting", "TSstudio", "ggplot2", "pastecs","readxl", "grid", "gridExtra", "kableExtra","knitr", "fpp2", "tstools", "tseries", "plotly", "scales","midasr"))
library(readr)
library(data.table)
library(psych)
library(dplyr)
library(tidyr)
library(reshape2)
library(lubridate)
library(forecast)
library(astsa)
library(zoo)
library(nowcasting)
library(TSstudio)
library(ggplot2)
library(pastecs)
library(readxl)
library(gridExtra)
library(grid)
library(kableExtra)
library(knitr)
library(fpp2)
library(tstools)
library(tseries)
library(plotly)
library(scales)
library(midasr)
```

```{r setup, include=FALSE, echo=FALSE}
require("knitr")
opts_knit$set(root.dir = "~/Documents/GitHub/RIDB/data")
```
#Forecasting

Get five campgrounds based on RIDB # of Sites
Blackwoods and Flamingo were on top of the list manifested odd occupancy, hence dropped

MATHER CAMPGROUND	Grand Canyon	328	307	Open Year Round	2	365
MORAINE PARK CAMPGROUND	Rocky Mountain	247	236	Open Year Round	4	365
UPPER PINES	Yosemite	238	235	Open Year Round	1	365
BIG MEADOWS	Shenandoah	221	221	Open March 30 - November 12	4	365
ELKMONT CAMPGROUND	Great Smoky Mountains	220	211	Open 3/9 - 11/25	4	365

##Elkmont from Great Smoky Mountains and Mather from Grand Canyon

Yosemite has the most number of campsites (902). However, only 84% is on RIDB (902 out of 1073). It has 8 campgrounds.

Mather is the biggest campground on RIDB.

Great Smoky has 17 campgrounds and 736 out of 750 cmapsites (second largest) on RIDB (98%). Elkmont is the biggest campground in Great Smoky.

```{r format campground data for forecasting}
rm(list=ls())

#load functions
source("~/Documents/GitHub/RIDB/formatCamp.R")
source("~/Documents/GitHub/RIDB/unitTest.R")
source("~/Documents/GitHub/RIDB/campMGraph.R")

#load data
data <- fread("~/Documents/GitHub/RIDB/data/final_data.csv")

#unique(data$id[data$Campground=="MATHER CAMPGROUND"]) #65
formatCamp(65)
matherD <- fread("descriptive_65.csv")
mather <- fread("campground_65.csv")
matherM <- fread("campground_monthly_65.csv")

#unique(data$id[data$Campground=="MORAINE PARK CAMPGROUND"]) #67
formatCamp(67)
moraineD <- fread("descriptive_67.csv")
moraine <- fread("campground_67.csv")
moraineM <- fread("campground_monthly_67.csv")

#unique(data$id[data$Campground=="UPPER PINES"]) #106
formatCamp(106)
upperD <- fread("descriptive_106.csv")
upper <- fread("campground_106.csv")
upperM <- fread("campground_monthly_106.csv")

#unique(data$id[data$Campground=="BIG MEADOWS"]) #7
formatCamp(7)
meadowsD <- fread("descriptive_7.csv")
meadows <- fread("campground_7.csv")
meadowsM <- fread("campground_monthly_7.csv")

#unique(data$id[data$Campground=="ELKMONT CAMPGROUND"]) #42
formatCamp(42)
elkmontD <- fread("descriptive_42.csv")
elkmont <- fread("campground_42.csv")
elkmontM <- fread("campground_monthly_42.csv")

#unique(data$id[data$Campground=="BLACKWOODS CAMPGROUND"]) #11
#formatCamp(11)
#black <- fread("campground_monthly_11.csv") #weird occupancy

#unique(data$id[data$Campground=="FLAMINGO"]) #45
#formatCamp(45)
#flamingo <- fread("campground_monthly_45.csv") #weird occupancy

#for consistency, start all camp data from 2007 May
elkmontM <- elkmontM[which(elkmontM$month>"2007-04-01")]
matherM <- matherM[which(matherM$month>"2007-04-01")]
meadowsM <- meadowsM[which(meadowsM$month>"2007-04-01")]
moraineM <- moraineM[which(moraineM$month>"2007-04-01")]
upperM <- upperM[which(upperM$month>"2007-04-01")]
```

```{r summary tables}
summary(meadowsD)
summary(elkmontD)
summary(matherD)
summary(moraineD)
summary(upperD)

#occupancy greater than 100%
dplyr::count(meadowsD[meadowsD$occupancy>1])/dplyr::count(meadowsD)
dplyr::count(elkmontD[elkmontD$occupancy>1])/dplyr::count(elkmontD)
dplyr::count(matherD[matherD$occupancy>1])/dplyr::count(matherD)
dplyr::count(moraineD[moraineD$occupancy>1])/dplyr::count(moraineD)
dplyr::count(upperD[upperD$occupancy>1])/dplyr::count(upperD)

#maximum occupancy
max(meadowsD$occupancy)
max(elkmontD$occupancy)
max(matherD$occupancy)
max(moraineD$occupancy)
max(upperD$occupancy)

rm(meadowsD, elkmontD, matherD, moraineD, upperD)
```


```{r occupancy graphs}
p1 <- campMGraph(mather, matherM)+ggtitle("Mather, Grand Canyon")
p2 <- campMGraph(elkmont, elkmontM)+ggtitle("Elkmont, Great Smoky Mountains")
p3 <- campMGraph(meadows, meadowsM)+ggtitle("Big Meadows, Shenandoah")
p4 <- campMGraph(moraine, moraineM)+ggtitle("Moraine, Rocky Mountain")
p5 <- campMGraph(upper, upperM)+ggtitle("Upper Pines, Yosemite")

jpeg('occupancy.jpg',
     width = 800, height = 450, units = "px", pointsize = 11,
     quality = 100)
grid.arrange(p1, p2, p3, p4, p5, ncol=3, nrow=2)
dev.off()
```


```{r Unit root tests}
## Unit root test
#install.packages(c("fUnitRoots", "uroot"))
library(fUnitRoots)
library(uroot)
library(urca)

unitTest(meadowsM)
unitTest(elkmontM)
unitTest(matherM)
unitTest(moraineM)
unitTest(upperM)

```

```{r, echo=FALSE}
#install.packages(c("feather", "rpart", "party", "randomForest"))
library(feather) # data import
library(rpart) # decision tree method
library(party) # decision tree method
library(randomForest) # ensemble learning method
library(ggplot2)
```

```{r ensemble}
n_date <- unique(matherM[, month]) #130
period <- 12

#I will use 118 months of data for training regression trees methods. Forecasts will be performed to one, three, six, and twelve months ahead. Let's extract train and test set from the dataset.
data_train <- matherM[month %in% n_date[1:118]]
data_test_1 <- matherM[month %in% n_date[119]]
data_test_3 <- matherM[month %in% n_date[119:120]]
data_test_6 <- matherM[month %in% n_date[119:123]]
data_test_12 <- matherM[month %in% n_date[119:130]]

data_train$month <- as.Date(data_train$month)
ggplot(data_train, aes(month, occupancy, group=1)) +
  geom_line() +
  labs(x = "Date", y = "Average Occupancy (%)")  + 
    theme(axis.title.y = element_text(size = 9)) + 
    theme(axis.title.x = element_text(size = 9))  +
    theme(panel.background = element_blank())   + 
    scale_y_continuous(breaks=seq(0,100,25))
```

#Bagging

Bagging or bootstrap aggregating, is ensemble learning meta-algorithm used to improve prediction accuracy and to overcome (avoid) overfitting. The algorithm is very simple. The first step is sampling a training dataset with replacement with some defined sample ratio (e.g. 0.7). Then a model is trained on a new train set. This procedure is repeated N_boot times (e.g. 100). Final ensemble prediction is just average of N_boot predictions. For aggregating predictions, the median can be also used and will be used in this post.

##Bagging + RPART

The first "bagged method is RPART (CART) tree. Training set consists of lagged reservation by one day and double-seasonal Fourier terms (monthly and yearly seasonality). Campsite reservation is firstly detrended by STL decomposition and trend part is forecasted (modeled) by ARIMA (auto.arima function). Seasonal and remainder part is then forecasted by regression tree model. More detailed description and explanations are in my previous post about regression trees methods. Let's define train and test data (by matrix_train and matrix_test).

```{r bagging}
data_ts <- ts(data_train$occupancy, freq = 12)
decomp_ts <- stl(data_ts, s.window = "periodic", robust = TRUE)$time.series
 
trend_part <- ts(decomp_ts[,2])
 
trend_fit <- auto.arima(trend_part) # ARIMA
trend_for <- as.vector(forecast(trend_fit, period)$mean) # trend forecast
 
data_msts <- msts(data_train$occupancy, seasonal.periods = c(period, period*12))
 
K <- 2
fuur <- fourier(data_msts, K = c(K, K)) # Fourier features to model (monthly and yearly)
 
N <- nrow(data_train)
window <- (N / period) - 1
 
new_load <- rowSums(decomp_ts[, c(1,3)]) # detrended original time series
lag_seas <- decomp_ts[1:(period*window), 1] # lag feature to model
 
matrix_train <- data.table(Load = tail(new_load, window*period),
                           fuur[(period + 1):N,],
                           Lag = lag_seas)
 
# create testing data matrix
test_lag <- decomp_ts[((period*window)+1):N, 1]
fuur_test <- fourier(data_msts, K = c(K, K), h = period)
 
matrix_test <- data.table(fuur_test,
                          Lag = test_lag)
```

I will perform 100 bootstrapped forecasts by RPART, that will be stored in the matrix of size 
100?48(pred_mat). Additional four parameters will be also sampled (randomized) to avoid overfitting. Sample ratio for sampling train set is sampled in the range from 0.7 to 0.9. Three hyperparameters of RPART are sampled also: minsplit, maxdepth and complexity parameter cp. Hyperparameters are sampled around values set on my previous post modeling.

```{r}
N_boot <- 100 # number of bootstraps
 
pred_mat <- matrix(0, nrow = N_boot, ncol = period)
for(i in 1:N_boot) {
  
  matrixSam <- matrix_train[sample(1:(N-period),
                                   floor((N-period) * sample(seq(0.7, 0.9, by = 0.01), 1)),
                                   replace = TRUE)] # sampling with sampled ratio from 0.7 to 0.9
  tree_bag <- rpart(Load ~ ., data = matrixSam,
                    control = rpart.control(minsplit = sample(2:3, 1),
                                            maxdepth = sample(26:30, 1),
                                            cp = sample(seq(0.0000009, 0.00001, by = 0.0000001), 1)))
  
  # new data and prediction
  pred_mat[i,] <- predict(tree_bag, matrix_test) + mean(trend_for)
}
```

Let's compute median of forecasts and visualize all created forecasts. For ggplot visualization needs we must use melt function to the matrix of forecasts pred_mat.

```{r}
pred_melt_rpart <- data.table(melt(pred_mat))
 
pred_ave_rpart <- pred_melt_rpart[, .(value = median(value)), by = .(Var2)]
pred_ave_rpart[, Var1 := "RPART_Bagg"]
 
ggplot(pred_melt_rpart, aes(Var2, value, group = Var1)) +
  geom_line(alpha = 0.75) +
  geom_line(data = pred_ave_rpart, aes(Var2, value), color = "firebrick2", alpha = 0.9, size = 2) +
  labs(x = "Time", y = "Load (kW)", title = "Bagging with RPART") + 
    theme(axis.title.y = element_text(size = 9)) + 
    theme(axis.title.x = element_text(size = 9))  +
    theme(panel.background = element_blank())   + 
    scale_y_continuous(breaks=seq(0,100,25))
```

The red line is median of forecasts. We can see that created forecasts by RPART have typical rectangular shape, but final ensemble forecasts has nice smooth behaviour. 

##Bagging with CTREE
The second "bagged" regression tree method is CTREE. I will randomize only mincriterion hyperparameter of CTREE method that decisions about splitting a node. The sample ratio is of course sampled (randomized) as well as in the previous case with RPART.

```{r}
pred_mat <- matrix(0, nrow = N_boot, ncol = period)
for(i in 1:N_boot) {
  
  matrixSam <- matrix_train[sample(1:(N-period),
                                   floor((N-period) * sample(seq(0.7, 0.9, by = 0.01), 1)),
                                   replace = TRUE)] # sampling with sampled ratio from 0.7 to 0.9
  tree_bag <- party::ctree(Load ~ ., data = matrixSam,
                           controls = party::ctree_control(teststat = c("quad"),
                                                           testtype = c("Teststatistic"),
                                                           mincriterion = sample(seq(0.88, 0.97, by = 0.005), 1),
                                                           minsplit = 1,
                                                           minbucket = 1,
                                                           mtry = 0, maxdepth = 0))
  
  # new data and prediction
  pred_mat[i,] <- predict(tree_bag, matrix_test) + mean(trend_for)
}

pred_melt_ctree <- data.table(melt(pred_mat))
 
pred_ave_ctree <- pred_melt_ctree[, .(value = median(value)), by = .(Var2)]
pred_ave_ctree[, Var1 := "CTREE_Bagg"]
 
ggplot(pred_melt_ctree, aes(Var2, value, group = Var1)) +
  geom_line(alpha = 0.75) +
  geom_line(data = pred_ave_ctree, aes(Var2, value), color = "firebrick2", alpha = 0.9, size = 2) +
  labs(x = "Time", y = "Load (kW)", title = "Bagging with CTREE")


rf_model <- randomForest(Load ~ ., data = data.frame(matrix_train),
                         ntree = 1000, mtry = 3, nodesize = 5, importance = TRUE)

pred_rf <- predict(rf_model, data.frame(matrix_test)) + mean(trend_for)

pred_rf <- data.table(value = pred_rf, Var2 = 1:12, Var1 = "RF")
 
pred_true <- data.table(value = data_test_12$occupancy, Var2 = 1:12, Var1 = "Real")

pred_avg <- cbind(pred_ave_rpart[,2], pred_ave_ctree[,2], pred_rf[,1])
pred_avg <- transform(pred_avg, average_value = rowMeans(pred_avg, na.rm = TRUE))
pred_avg[,1:3] <-NULL
colnames(pred_avg) <- c("value")
pred_avg <- data.table(value = pred_avg$value, Var2 = 1:12, Var1 = "Average")

preds_all <- rbindlist(list(pred_ave_rpart, pred_ave_ctree, pred_rf, pred_avg, pred_true), use.names = T)
 
ggplot(preds_all, aes(Var2, value, color = as.factor(Var1))) +
  geom_line(alpha = 0.7, size = 1.2) +
  labs(x = "Time", y = "Load (kW)", title = "Comparison of Ensemble Learning forecasts") +
  guides(color=guide_legend(title="Method")) 
```


```{r Moving Average}
source("~/Documents/GitHub/RIDB/maProcess.R")
maProcess(elkmontM, 3)
maProcess(matherM, 3)
maProcess(meadowsM, 3)
maProcess(moraineM, 3)
maProcess(upperM, 3)
```

Our data is inevitably seasonal. Hence, we will run seasonal ARIMA model as the first step of analysis. Refer to https://onlinecourses.science.psu.edu/stat510/node/67/ for more information.
We will test time-series analysis with one ID. We will run each ID separately as each facility has different length of time.

Some useful sources for forecasting with R:
http://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html

https://robjhyndman.com/hyndsight/dailydata/

```{r ARIMA}
source("~/Documents/GitHub/RIDB/arimaProcess.R")
#arimaProcess(dataname, number of forecasts)

arimaProcess(elkmontM, 12)
arimaProcess(matherM, 12)
arimaProcess(meadowsM, 12)
arimaProcess(moraineM, 12)
arimaProcess(upperM, 12)
```

```{r SARIMA}
source("~/Documents/GitHub/RIDB/sarimaProcess.R")
sarimaProcess(elkmontM, 12)
sarimaProcess(matherM, 12)
sarimaProcess(meadowsM, 12)
sarimaProcess(moraineM, 12)
sarimaProcess(upperM, 12)

jpeg(filename = "sarima.jpeg", width = 10, height = 4, units = 'in', res = 300)
grid_arrange_shared_legend(a, b, ncol=2)
dev.off()  
```

```{r ETS}
occupancy <- ts(matherM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)

ETS <- forecast(ets(train), h=h)
forecast::accuracy(ETS$model$fitted, occupancy)

fitted <- ETS$model$fitted
e <- abs((occupancy[1:106]-fitted)/occupancy[1:106]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

a <- forecast::autoplot(occupancy, series="Data") +
      autolayer(ETS, series="ETS", alpha=0.5, size=2) +
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      xlab("") + ylab("Occupancy (%)")  + 
      ggtitle("Mather Campground")


occupancy <- ts(elkmontM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)

ETS <- forecast(ets(train), h=h)
forecast::accuracy(ETS$model$fitted, occupancy)

fitted <- ETS$model$fitted
e <- abs((occupancy[1:106]-fitted)/occupancy[1:106]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

b <- autoplot(occupancy) +
      autolayer(ETS, series="ETS", alpha=0.5) +
      xlab("") + ylab("")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      ggtitle("Elkmont Campground")

jpeg(filename = "ets.jpeg", width = 10, height = 4, units = 'in', res = 300)
grid_arrange_shared_legend(a, b, ncol=2)
dev.off()  

```

```{r}
occupancy <- ts(matherM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)
ARIMA <- forecast(auto.arima(train, 
                                  trace = FALSE, 
                                  ic = "bic", 
                                  test = "adf",
                                  approximation= FALSE,
                                  allowmean = TRUE,
                                  allowdrift = TRUE), h=h)
ETS <- forecast(ets(train), h=h)
NNAR <- forecast(nnetar(train, P=8), h=h)

Combination <- (ETS[["mean"]] + ARIMA[["mean"]] +
  NNAR[["mean"]])/3

forecast::accuracy(Combination, occupancy)
fitted <- Combination
e <- abs((occupancy[107:129]-fitted)/occupancy[107:129]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

abs((occupancy[107:129]-fitted))/occupancy[107:129]

a<- autoplot(occupancy) +
        autolayer(Combination, series="Combination", alpha=0.5) +
      xlab("") + ylab("Occupancy (%)")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
  ggtitle("Mather Campground")


occupancy <- ts(elkmontM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)
ARIMA <- forecast(auto.arima(train, 
                                  trace = FALSE, 
                                  ic = "bic", 
                                  test = "adf",
                                  approximation= FALSE,
                                  allowmean = TRUE,
                                  allowdrift = TRUE), h=h)
ETS <- forecast(ets(train), h=h)
NNAR <- forecast(nnetar(train, P=8),h=h)

Combination <- (ETS[["mean"]] + ARIMA[["mean"]] +
  NNAR[["mean"]])/3

forecast::accuracy(Combination, occupancy)
fitted <- Combination
e <- abs((occupancy[107:128]-fitted)/occupancy[107:128]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)
e

b <- autoplot(occupancy) +
      autolayer(ETS, series="Combination", alpha=0.5) +
      xlab("") + ylab("")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      ggtitle("Elkmont Campground")

jpeg(filename = "combination.jpeg", width = 10, height = 4, units = 'in', res = 300)
grid_arrange_shared_legend(a, b, ncol=2)
dev.off()  
```

```{r}
# Fit MLP
mlp.fit <- mlp(ts(elkmontM$occupancy), m=12, lags=12, outplot = TRUE)
jpeg('avgOccu_NN.jpg')
plot(mlp.fit)
dev.off()
print(mlp.fit)

occupancy <- ts(matherM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)
fit <- nnetar(train, lambda = "auto", repeats = 100)
autoplot(forecast(fit, h=h))
nnetar(train, P=8, lambda = "auto", repeats = 100)
NNAR <- forecast(nnetar(train, P=8, lambda = "auto", repeats = 100), h=h)

forecast::accuracy(NNAR$fitted, occupancy)

fitted <- NNAR$fitted
e <- abs((occupancy[1:106]-fitted)/occupancy[1:106]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

a <- autoplot(occupancy) +
      autolayer(NNAR, series="NNAR", alpha=0.5) +
      xlab("") + ylab("Occupancy (%)")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      ggtitle("Mather Campground")

occupancy <- ts(elkmontM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)
fit <- nnetar(train, lambda = "auto", repeats = 100)
autoplot(forecast(fit, h=h))
nnetar(train, P=8, lambda = "auto", repeats = 100)
NNAR <- forecast(nnetar(train, P=8, lambda = "auto", repeats = 100), h=h)

forecast::accuracy(NNAR$fitted, occupancy)

fitted <- NNAR$fitted
e <- abs((occupancy[1:106]-fitted)/occupancy[1:106]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

b <- autoplot(occupancy) +
      autolayer(NNAR, series="NNAR", alpha=0.5) +
      xlab("") + ylab("Occupancy (%)")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      ggtitle("Elkmont Campground")

jpeg(filename = "nnar.jpeg", width = 10, height = 4, units = 'in', res = 300)
grid_arrange_shared_legend(a, b, ncol=2)
dev.off()  




```

```{r}
occupancy <- ts(matherM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)

arf <- arfima(train, order = c(4,0,1), numeach = c(3,3))



spec = ugarchspec()
garch <- ugarchfit(train, spec=spec)
GARCH <- ugarchforecast(garch, n.ahead=h)

forecast::accuracy(GARCH@forecast$seriesFor, occupancy)

fitted <- GARCH@forecast$seriesFor
e <- abs((occupancy[1:106]-fitted)/occupancy[1:106]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

a <- autoplot(occupancy) +
      autolayer(GARCH@forecast$seriesFor, series="NNAR", PI=FALSE) +
      xlab("Year") + ylab("Occupancy (%)")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      ggtitle("Mather Campground")

occupancy <- ts(elkmontM$occupancy, frequency = 12, start = c(2007, 3))
train <- window(occupancy, end=c(2015,12))
h <- length(occupancy) - length(train)
NNAR <- forecast(nnetar(train), h=h)

forecast::accuracy(NNAR$fitted, occupancy)

fitted <- NNAR$fitted
e <- abs((occupancy[1:106]-fitted)/occupancy[1:106]) * 100
e <- e[!is.na(e)]
e <- e[e!=Inf]
mean(e)

b <- autoplot(occupancy) +
      autolayer(NNAR, series="NNAR", PI=FALSE) +
      xlab("") + ylab("Occupancy (%)")  + 
  theme(panel.background = element_blank()) + 
  scale_y_continuous(breaks=seq(0,100,25)) +
      ggtitle("Elkmont Campground")

jpeg(filename = "nnar.jpeg", width = 10, height = 4, units = 'in', res = 300)
grid_arrange_shared_legend(a, b, ncol=2)
dev.off()  
```

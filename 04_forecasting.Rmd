---
title: "04_forecasting"
---
---
title: "Forecasting"
---

```{r quietly=T}
#install.packages(c("readr", "data.table", "psych","dplyr", "tidyr", "reshape2", "lubridate", "forecast", "astsa", "zoo", "nowcasting", "TSstudio", "ggplot2", "pastecs","readxl", "grid", "gridExtra", "kableExtra","knitr", "fpp2", "tstools", "tseries", "plotly", "scales","midasr"))
library(readr)
library(data.table)
library(psych)
library(dplyr)
library(tidyr)
library(reshape2)
library(lubridate)
library(forecast)
library(astsa)
library(zoo)
library(nowcasting)
library(TSstudio)
library(ggplot2)
library(pastecs)
library(readxl)
library(gridExtra)
library(grid)
library(kableExtra)
library(knitr)
library(fpp2)
library(tstools)
library(tseries)
library(plotly)
library(scales)
library(midasr)
```

#Forecasting

Get five campgrounds based on RIDB # of Sites
Blackwoods and Flamingo were on top of the list manifested odd occupancy, hence dropped

MATHER CAMPGROUND	Grand Canyon	328	307	Open Year Round	2	365
MORAINE PARK CAMPGROUND	Rocky Mountain	247	236	Open Year Round	4	365
UPPER PINES	Yosemite	238	235	Open Year Round	1	365
BIG MEADOWS	Shenandoah	221	221	Open March 30 - November 12	4	365
ELKMONT CAMPGROUND	Great Smoky Mountains	220	211	Open 3/9 - 11/25	4	365

##Elkmont from Great Smoky Mountains and Mather from Grand Canyon

Yosemite has the most number of campsites (902). However, only 84% is on RIDB (902 out of 1073). It has 8 campgrounds.

Mather is the biggest campground on RIDB.

Great Smoky has 17 campgrounds and 736 out of 750 cmapsites (second largest) on RIDB (98%). Elkmont is the biggest campground in Great Smoky.

```{r format campground data for forecasting}
setwd("~/Documents/GitHub/RIDB/data")
data <- fread("~/Documents/GitHub/RIDB/data/final_data.csv")

#unique(data$id[data$Campground=="MATHER CAMPGROUND"]) #65
formatCamp(65)
matherD <- fread("descriptive_65.csv")
mather <- fread("campground_65.csv")
matherM <- fread("campground_monthly_65.csv")

#unique(data$id[data$Campground=="MORAINE PARK CAMPGROUND"]) #67
formatCamp(67)
moraineD <- fread("descriptive_67.csv")
moraine <- fread("campground_67.csv")
moraineM <- fread("campground_monthly_67.csv")

#unique(data$id[data$Campground=="UPPER PINES"]) #106
formatCamp(106)
upperD <- fread("descriptive_106.csv")
upper <- fread("campground_106.csv")
upperM <- fread("campground_monthly_106.csv")

#unique(data$id[data$Campground=="BIG MEADOWS"]) #7
formatCamp(7)
meadowsD <- fread("descriptive_7.csv")
meadows <- fread("campground_7.csv")
meadowsM <- fread("campground_monthly_7.csv")

#unique(data$id[data$Campground=="ELKMONT CAMPGROUND"]) #42
formatCamp(42)
elkmontD <- fread("descriptive_42.csv")
elkmont <- fread("campground_42.csv")
elkmontM <- fread("campground_monthly_42.csv")

#unique(data$id[data$Campground=="BLACKWOODS CAMPGROUND"]) #11
#formatCamp(11)
#black <- fread("campground_monthly_11.csv") #weird occupancy

#unique(data$id[data$Campground=="FLAMINGO"]) #45
#formatCamp(45)
#flamingo <- fread("campground_monthly_45.csv") #weird occupancy
```

```{r summary tables}
summary(meadowsD)
summary(elkmontD)
summary(matherD)
summary(moraineD)
summary(upperD)

#occupancy greater than 100%
dplyr::count(meadowsD[meadowsD$occupancy>1])/dplyr::count(meadowsD)
dplyr::count(elkmontD[elkmontD$occupancy>1])/dplyr::count(elkmontD)
dplyr::count(matherD[matherD$occupancy>1])/dplyr::count(matherD)
dplyr::count(moraineD[moraineD$occupancy>1])/dplyr::count(moraineD)
dplyr::count(upperD[upperD$occupancy>1])/dplyr::count(upperD)

#maximum occupancy
max(meadowsD$occupancy)
max(elkmontD$occupancy)
max(matherD$occupancy)
max(moraineD$occupancy)
max(upperD$occupancy)
```


```{r occupancy graphs}
p1 <- campMGraph(mather, matherM)+ggtitle("Mather, Grand Canyon")
p2 <- campMGraph(elkmont, elkmontM)+ggtitle("Elkmont, Great Smoky Mountains")
p3 <- campMGraph(meadows, meadowsM)+ggtitle("Big Meadows, Shenandoah")
p4 <- campMGraph(moraine, moraineM)+ggtitle("Moraine, Rocky Mountain")
p5 <- campMGraph(upper, upperM)+ggtitle("Upper Pines, Yosemite")

jpeg('occupancy.jpg',
     width = 800, height = 450, units = "px", pointsize = 11,
     quality = 100)
grid.arrange(p1, p2, p3, p4, p5, ncol=3, nrow=2)
dev.off()
```


```{r Unit root tests}
## Unit root test
install.packages(c("fUnitRoots", "uroot"))
library(fUnitRoots)
library(uroot)

unitTest(matherM)
occupancy <- ts(matherM$occupancy, frequency = 12)
a <- ocsb.test(occupancy, lag.method = c("fixed"), maxlag = 12)
unitTest(elkmontM)
unitTest(meadowsM)
unitTest(moraineM)
unitTest(upperM)
```

```{r}
#install.packages(c("feather", "rpart", "party", "randomForest"))
library(feather) # data import
library(rpart) # decision tree method
library(party) # decision tree method
library(randomForest) # ensemble learning method
library(ggplot2)
```

```{r ensemble}
n_date <- unique(matherM[, month]) #130
period <- 

#I will use 118 months of data for training regression trees methods. Forecasts will be performed to one, three, six, and twelve months ahead. Let’s extract train and test set from the dataset.
data_train <- matherM[month %in% n_date[1:118]]
data_test_1 <- matherM[month %in% n_date[119]]
data_test_3 <- matherM[month %in% n_date[119:120]]
data_test_6 <- matherM[month %in% n_date[119:123]]
data_test_12 <- matherM[month %in% n_date[119:130]]

data_train$month <- as.Date(data_train$month)
ggplot(data_train, aes(month, occupancy, group=1)) +
  geom_line() +
  labs(x = "Date", y = "Average Occupancy (%)")  + 
    theme(axis.title.y = element_text(size = 9)) + 
    theme(axis.title.x = element_text(size = 9))  +
    theme(panel.background = element_blank())   + 
    scale_y_continuous(breaks=seq(0,100,25))
```

#Bagging

Bagging or bootstrap aggregating, is ensemble learning meta-algorithm used to improve prediction accuracy and to overcome (avoid) overfitting. The algorithm is very simple. The first step is sampling a training dataset with replacement with some defined sample ratio (e.g. 0.7). Then a model is trained on a new train set. This procedure is repeated N_boot times (e.g. 100). Final ensemble prediction is just average of N_boot predictions. For aggregating predictions, the median can be also used and will be used in this post.

##Bagging + RPART

The first “bagged” method is RPART (CART) tree. Training set consists of lagged electricity load by one day and double-seasonal Fourier terms (daily and weekly seasonality). Electricity consumption is firstly detrended by STL decomposition and trend part is forecasted (modeled) by ARIMA (auto.arima function). Seasonal and remainder part is then forecasted by regression tree model. More detailed description and explanations are in my previous post about regression trees methods. Let’s define train and test data (by matrix_train and matrix_test).

```{r bagging}
data_ts <- ts(data_train$occupancy, freq = period)
decomp_ts <- stl(data_ts, s.window = "periodic", robust = TRUE)$time.series
 
trend_part <- ts(decomp_ts[,2])
 
trend_fit <- auto.arima(trend_part) # ARIMA
trend_for <- as.vector(forecast(trend_fit, period)$mean) # trend forecast
 
data_msts <- msts(data_train$occupancy, seasonal.periods = c(period, period*7))
 
K <- 2
fuur <- fourier(data_msts, K = c(K, K)) # Fourier features to model (Daily and Weekly)
 
N <- nrow(data_train)
window <- (N / period) - 1
 
new_load <- rowSums(decomp_ts[, c(1,3)]) # detrended original time series
lag_seas <- decomp_ts[1:(period*window), 1] # lag feature to model
 
matrix_train <- data.table(Load = tail(new_load, window*period),
                           fuur[(period + 1):N,],
                           Lag = lag_seas)
 
# create testing data matrix
test_lag <- decomp_ts[((period*window)+1):N, 1]
fuur_test <- fourier(data_msts, K = c(K, K), h = period)
 
matrix_test <- data.table(fuur_test,
                          Lag = test_lag)
```

